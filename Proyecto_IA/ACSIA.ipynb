{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import assemblyai as aai\n",
    "\n",
    "aai.settings.api_key = \"46f3fc0a6e364888acf89978f15a5d24\"\n",
    "transcriber = aai.Transcriber()\n",
    "\n",
    "transcript = transcriber.transcribe(\"onlymp3.to - Speech recognition in Python made easy Python Tutorial-YdYTSxEW5bA-192k-1700264266.mp3\")\n",
    "\n",
    "oraciones= transcript.get_paragraphs()\n",
    "\n",
    "for parrafo in oraciones:\n",
    "    print(parrafo.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import assemblyai as aai\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribir_audio(api_key, audio_path):\n",
    "    aai.settings.api_key = api_key\n",
    "    transcriber = aai.Transcriber()\n",
    "    transcript = transcriber.transcribe(audio_path)\n",
    "    oraciones = transcript.get_paragraphs()\n",
    "    oraciones_texto = [parrafo.text for parrafo in oraciones]\n",
    "    texto_transcrito = ' '.join(oraciones_texto)\n",
    "    return texto_transcrito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_respuesta(prompt, modelo, tokenizer, max_length=500, min_length=20):\n",
    "    entrada_codificada = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    salida = modelo.generate(\n",
    "        entrada_codificada,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        num_beams=10,\n",
    "        use_cache= True,\n",
    "        no_repeat_ngram_size=1,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    texto_generado = tokenizer.decode(salida[0], skip_special_tokens=True)\n",
    "    texto_generado = texto_generado.strip()\n",
    "    return texto_generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi everyone, this is Patrick from Assemblyai. And in this video we're going to take a look at our Python SDK. So I show you how you can transcribe and analyze audio data with just a few lines of code. So let's get started. So first let's set up the Python SDK and you can find this on GitHub by the way. And to install this you can simply say PIP, install assemblyai. And now let's jump to the code. And the first thing we want to do is import and then you want to set your API key and you can do this by calling AAI settings API key and here set your key. You can get one for free on our website. The link will be in the description below. Or as a second way, you could also set this as a environment variable that has to be called Assemblyai ApI Key. For example, if I open my terminal I could hear export assemblyai API key and set this as key. And then you don't need this line, but here we want to do it with the first option. Next, let's learn how we can transcribe files with the SDK. So for this we create a transcriber instance and then we can call transcriber transcribe. And this works with either a URL to a file, or we can also simply pass in a local file. So here I prepared a local MP3 file and now this will start the transcription and this function will block until the transcription is completed. And then finally we can simply call transcript text to see the transcribe text. So let's comment this one out and use the local file. And let's run this and see if it works. And as you can see here, we get the transcription. Now this transcript object is of the class transcriber transcript, and you can do many more things with this. For example, you could check the audio duration, or you can get the single sentences by calling transcript, get sentences, and then we can iterate over them and print the text again. And as you can see here we get the single sentences, or instead of the sentences you can also call transcript, get paragraphs and then iterate over them. And here we get the single paragraphs. You can also do a word search. For example you can call transcript word search and here as a list you can pass in all the words you want to search for. And then you get the matches back so we can iterate over them. And then for example we can print found, match text and then match count. And then you can also get the timestamps for each word and also the indices. So let's run this and see if it works. And as you can see, it found President two times. And here we get the corresponding timestamp and the indices. And it also found people one time. And here is the corresponding timestamp. Now let's learn how we can transcribe files asynchronous. So as I've mentioned, the transcriber transcribe function will block until the result is finished. So we can also call transcribeasync. And now this will return a future object from the async I O library. So let's run this, and this will immediately return. And then we can do some other stuff. So as you can see, this immediately returns and we can see the state is running. And then at a later time, for example, we can check if the future is done. And then we can get the transcript by calling future result. And now this is again the same transcript object that we've seen before. For example, here we can now print transcript text. And by now, hopefully the state is already completed. So now here we should see the text. And as you can see, this worked. So this is how you can transcribe files asynchronously. Now let's learn how you can configure transcription parameters and also trigger different audio intelligence features. So for this you can set up a transcription config. And here you can set different parameters. For example, you can set punctuate should be false and formatting should be false. And then you pass this config when you set up your transcriber object. So this is the first way how you could do it. You can also directly call this on the transcribe function. Now we do it with the global configuration. And then again you call transcriber transcribe. And now we print the transcription text. And let's run this and wait until this is done. And here again it prints the transcript, but this time without punctuation and formatting. You can also use the transcription config to set up different audio intelligence features. For example, let's use the summarization feature. So when we set up the transcription config, here we set summarization equals true. And then you could also optionally set the summary model and the summary type. And here we set this to informative and then to bullets. So this is the second way how we can use it. We can directly call this on the transcribe function, and then it will overwrite the global config. And then since we set summarization equals true, we can then access transcript summary. So let's run this, and as you can see now we get a summary with two bullet points. So this was a short guide how you can get started with the Python SDK. I recommend to check out the documentation to see what else you can do with it, and I hope you enjoyed this video. So if so, then please leave us a like and consider subscribing to our channel. And then I hope to see you next time. Bye.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respuesta generada:\n",
      "Tell me about the content of the transcribed text. Give me the summary of the text, and I'll send you an email with a link back to this page if it's good enough for your reading pleasure!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    api_key_assemblyai = \"46f3fc0a6e364888acf89978f15a5d24\"\n",
    "    audio_path = \"onlymp3.to - Speech recognition in Python made easy Python Tutorial-YdYTSxEW5bA-192k-1700264266.mp3\"  \n",
    "\n",
    "    texto_transcrito = transcribir_audio(api_key_assemblyai, audio_path)\n",
    "\n",
    "    print(texto_transcrito)\n",
    "\n",
    "    modelo_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    pregunta = \"\\nTell me about the content of the transcribed text. Give me the summary of the text\"\n",
    "\n",
    "    respuesta_generada = generar_respuesta(pregunta, modelo_gpt2, tokenizer_gpt2)\n",
    "\n",
    "    print(\"\\nRespuesta generada:\")\n",
    "    print(respuesta_generada)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi everyone, this is Patrick from AssemblyAI, and in this video, we're going to take a look at our Python SDK. So I show you how you can transcribe and analyze audio data with just a few lines of code. So let's get started. So first let's set up the Python SDK. And you can find this on GitHub by the way. And to install this, you can simply say pip install AssemblyAI and now let's jump to the code. And the first thing we want to do is import. And then you want to set your API key. And you can do this by calling AAI Settings API key. And here set your key. You can get one for free on our website. The link will be in the description below. Or as a second way, you could also set this as a environment variable that has to be called AssemblyAI API key. For example, if I open my terminal, I could hear export AssemblyAI API key and set this as key and then you don't need this line. But here we want to do it with the first option. Next, let's learn how we can transcribe files with the SDK. So for this we create a transcriber instance and then we can call transcriber transcribe. And this works with either a URL to a file, or we can also simply pass in a local file. So here I prepared a local MP3 file and now this will start the transcription and this function will block until the transcription is completed. And then finally we can simply call Transcript text to see the transcribe text. So let's comment this one out and use the local file and let's run this and see if it works. And as you can see here we get the transcription. Now, this transcript object is of the class Transcriber transcript and you can do many more things with this. For example, you could check the audio duration, or you can get the single sentences by calling Transcript Get Sentences and then we can iterate over them and print the text again. And as you can see here we get the single sentences, or instead of the sentences, you can also call Transcript Get Paragraphs and then iterate over them. And here we get the single paragraphs. You can also do a word search. For example, you can call Transcript Word Search and here as a list, you can pass in all the words you want to search for. And then you get the matches back so we can iterate over them. And then for example, we can print Found Match text and then Match Count. And then you can also get the timestamps for each word and also the indices. So let's run this and see if it works. And as you can see, it Found President two times. And here we get the corresponding timestamp and the indices and it also found people one time and here is the corresponding timestamp. Now let's learn how we can transcribe files asynchronous. So as I've mentioned, the transcriber transcribe function will block until the result is finished. So we can also call transcribeasync. And now this will return a future object from the Async I O library. So let's run this and this will immediately return and then we can do some other stuff. So as you can see, this immediately returns and we can see the state is running. And then at a later time, for example, we can check if the future is done and then we can get the transcript by calling future result. And now this is again the same transcript object that we've seen before. For example, here we can now print transcript text, and by now hopefully the state is already completed. So now here we should see the text and as you can see this worked. So this is how you can transcribe files asynchronously now let's learn how you can configure transcription parameters and also trigger different audio intelligence features. So for this you can set up a transcription config and here you can set different parameters. For example, you can set punctuate should be false and formatting should be false. And then you pass this config when you set up your transcriber object. So this is the first way how you could do it. You can also directly call this on the transcribe function. Now we do it with the global configuration. And then again you call transcriber transcribe and now we print the transcription text and let's run this and wait until this is done. And here again it prints the transcript, but this time without punctuation and formatting. You can also use the transcription config to set up different audio intelligence features. For example, let's use the summarization feature. So when we set up the transcription config here we set summarization equals true. And then you could also optionally set the summary model and the summary type. And here we set this to informative and then to bullets. So this is the second way how we can use it. We can directly call this on the transcribe function and then it will overwrite the global config. And then since we set summarization equals true, we can then access transcript summary. So let's run this and as you can see, now we get a summary with two bullet points. So this was a short guide how you can get started with the Python SDK. I recommend to check out the documentation to see what else you can do with it. And I hope you enjoyed this video. So if so then please leave us a like and consider subscribing to our channel. And then I hope to see you next time. Bye.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 571/571 [00:00<?, ?B/s] \n",
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\andre\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 1.34G/1.34G [00:38<00:00, 35.2MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.51MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 11.2MB/s]\n",
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi everyone, this is Patrick from AssemblyAI, and in this video, we're going to take a look at our Python SDK. So I show you how you can transcribe and analyze audio data with just a few lines of code. And to install this, you can simply say pip install AssemblyAI and now let's jump to the code. And this works with either a URL to a file, or we can also simply pass in a local file. And now this is again the same transcript object that we've seen before. And then you pass this config when you set up your transcriber object. And then again you call transcriber transcribe and now we print the transcription text and let's run this and wait until this is done. So when we set up the transcription config here we set summarization equals true. I recommend to check out the documentation to see what else you can do with it.\n"
     ]
    }
   ],
   "source": [
    "import assemblyai as aai\n",
    "from summarizer import Summarizer,TransformerSummarizer\n",
    "\n",
    "def transcribir_audio(api_key, audio_path):\n",
    "    aai.settings.api_key = api_key\n",
    "    transcriber = aai.Transcriber()\n",
    "    transcript = transcriber.transcribe(audio_path)\n",
    "    oraciones = transcript.get_paragraphs()\n",
    "    oraciones_texto = [parrafo.text for parrafo in oraciones]\n",
    "    texto_transcrito = ' '.join(oraciones_texto)\n",
    "    return texto_transcrito\n",
    "\n",
    "def main():\n",
    "    api_key_assemblyai = \"46f3fc0a6e364888acf89978f15a5d24\"\n",
    "    audio_path = \"onlymp3.to - Speech recognition in Python made easy Python Tutorial-YdYTSxEW5bA-192k-1700264266.mp3\"  \n",
    "\n",
    "    texto_transcrito = transcribir_audio(api_key_assemblyai, audio_path)\n",
    "\n",
    "    print(texto_transcrito)\n",
    "\n",
    "    bert_model = Summarizer()\n",
    "    bert_summary = ''.join(bert_model(texto_transcrito, min_length=60))\n",
    "    print(bert_summary) \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
